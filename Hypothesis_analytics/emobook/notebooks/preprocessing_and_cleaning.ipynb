{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ee5829-4d88-4821-9f14-3b5dae60ef4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Frankenstein.txt → Frankenstein.clean.txt (contents_removed=True)\n",
      "✔ Mobi Dick.txt → Mobi Dick.clean.txt (contents_removed=True)\n",
      "✔ Pride and Prejudice.txt → Pride and Prejudice.clean.txt (contents_removed=False)\n",
      "✔ Romeo and Juliet.txt → Romeo and Juliet.clean.txt (contents_removed=True)\n",
      "✔ The Adventures of Sherlock Holmes.txt → The Adventures of Sherlock Holmes.clean.txt (contents_removed=True)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re, unicodedata, pandas as pd\n",
    "\n",
    "IN_DIR  = Path(\"../books\")       # run from emobook/notebooks/\n",
    "OUT_DIR = Path(\"../books_clean\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- core: extract story between PG START/END (prefer modern START) ----\n",
    "MODERN_START = re.compile(r\"^\\s*\\*\\*\\*\\s*start of (?:this|the) project gutenberg ebook.*$\", re.I|re.M)\n",
    "LEGACY_START = re.compile(r\"^\\s*the project gutenberg e?book of .*$\", re.I|re.M)\n",
    "END_RES = [re.compile(p, re.I|re.M) for p in [\n",
    "    r\"^\\s*\\*\\*\\*\\s*end of (?:this|the) project gutenberg ebook.*$\",\n",
    "    r\"^\\s*end of (?:this|the) project gutenberg ebook.*$\",\n",
    "    r\"^\\s*end of project gutenberg'?s .*?$\",\n",
    "    r\"^\\s*end of the project gutenberg ebook of .*$\",\n",
    "    r\"^\\s*project gutenberg(?:™|) license.*$\",\n",
    "]]\n",
    "\n",
    "def extract_gutenberg_story(text: str) -> str:\n",
    "    t = text.replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\")\n",
    "    if t.startswith(\"\\ufeff\"):  # strip BOM\n",
    "        t = t.lstrip(\"\\ufeff\")\n",
    "\n",
    "    m = MODERN_START.search(t)\n",
    "    if m:\n",
    "        # jump to the newline AFTER the marker line\n",
    "        start_idx = t.find(\"\\n\", m.end())\n",
    "        start_idx = start_idx + 1 if start_idx != -1 else m.end()\n",
    "    else:\n",
    "        mh = LEGACY_START.search(t)\n",
    "        if mh:\n",
    "            start_idx = t.find(\"\\n\", mh.end())\n",
    "            start_idx = start_idx + 1 if start_idx != -1 else mh.end()\n",
    "            # prefer a later modern START if present\n",
    "            m2 = MODERN_START.search(t, pos=start_idx)\n",
    "            if m2:\n",
    "                start_idx = t.find(\"\\n\", m2.end())\n",
    "                start_idx = start_idx + 1 if start_idx != -1 else m2.end()\n",
    "        else:\n",
    "            start_idx = 0  # no markers, pass through from start\n",
    "\n",
    "    # earliest END marker AFTER start\n",
    "    ends = [m.start() for R in END_RES if (m := R.search(t, pos=start_idx))]\n",
    "    end_idx = min(ends) if ends else len(t)\n",
    "    return t[start_idx:end_idx].lstrip(\"\\n\")\n",
    "\n",
    "# --- optional: drop 'Contents' block safely (handles roman item lists like \"I. ...\") ---\n",
    "ROMAN_ITEM = re.compile(r\"(?:^|\\n)\\s*[IVXLCDM]+\\.\\s\", re.M)\n",
    "CHAP_OR_SCENE = re.compile(r\"(?:^|\\n)\\s*(?:chapter|book|part|canto|volume|act|scene)\\s+[ivxlcdm0-9]+\\b\", re.I|re.M)\n",
    "\n",
    "def drop_contents_block(core: str):\n",
    "    m = re.search(r\"(?:^|\\n)\\s*contents\\s*\\n\", core[:200_000], re.I)\n",
    "    if not m:\n",
    "        return core, False\n",
    "    after = core[m.end():]\n",
    "    c1 = CHAP_OR_SCENE.search(after)\n",
    "    c2 = ROMAN_ITEM.search(after)\n",
    "    idxs = [x.start() for x in (c1, c2) if x]\n",
    "    return (after[min(idxs):], True) if idxs else (core, False)\n",
    "\n",
    "# --- normalization (minimal, tokenizer-friendly) ---\n",
    "def normalize_basic(txt: str) -> str:\n",
    "    txt = unicodedata.normalize(\"NFC\", txt)\n",
    "    txt = txt.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "    txt = re.sub(r\"[ \\t]+\", \" \", txt)      # collapse spaces\n",
    "    txt = re.sub(r\"\\n{3,}\", \"\\n\\n\", txt)   # collapse blank lines\n",
    "    return txt.strip()\n",
    "\n",
    "# --- process all books in ../books ----------------------------------------\n",
    "def preprocess_all(in_dir: Path, out_dir: Path, remove_contents=True):\n",
    "    rows = []\n",
    "    for p in sorted(in_dir.glob(\"*.txt\")):\n",
    "        raw = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        core = extract_gutenberg_story(raw)\n",
    "\n",
    "        contents_removed = False\n",
    "        if remove_contents:\n",
    "            core, contents_removed = drop_contents_block(core)\n",
    "\n",
    "        cleaned = normalize_basic(core)\n",
    "        outp = out_dir / (p.stem + \".clean.txt\")\n",
    "        outp.write_text(cleaned, encoding=\"utf-8\")\n",
    "\n",
    "        rows.append({\n",
    "            \"file\": p.name,\n",
    "            \"chars_raw\": len(raw),\n",
    "            \"chars_clean\": len(cleaned),\n",
    "            \"reduction_%\": round(100*(1 - len(cleaned)/max(1, len(raw))), 2),\n",
    "            \"contents_removed\": contents_removed\n",
    "        })\n",
    "        print(f\"✔ {p.name} → {outp.name} (contents_removed={contents_removed})\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(out_dir / \"preprocess_report.csv\", index=False)\n",
    "    df\n",
    "\n",
    "_ = preprocess_all(IN_DIR, OUT_DIR, remove_contents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c3675b-f25f-4cff-bfb5-5becba13f291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>has_pg_marker</th>\n",
       "      <th>has_license</th>\n",
       "      <th>contents_in_head</th>\n",
       "      <th>has_illustrations</th>\n",
       "      <th>stage_dir_count</th>\n",
       "      <th>act_scene_cues</th>\n",
       "      <th>speaker_lines</th>\n",
       "      <th>chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frankenstein.clean.txt</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>419006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mobi Dick.clean.txt</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1218030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pride and Prejudice.clean.txt</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>721387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Romeo and Juliet.clean.txt</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>123</td>\n",
       "      <td>58</td>\n",
       "      <td>840</td>\n",
       "      <td>142317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Adventures of Sherlock Holmes.clean.txt</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>561782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          file  has_pg_marker  has_license  \\\n",
       "0                       Frankenstein.clean.txt          False        False   \n",
       "1                          Mobi Dick.clean.txt           True        False   \n",
       "2                Pride and Prejudice.clean.txt          False        False   \n",
       "3                   Romeo and Juliet.clean.txt          False        False   \n",
       "4  The Adventures of Sherlock Holmes.clean.txt          False        False   \n",
       "\n",
       "   contents_in_head  has_illustrations  stage_dir_count  act_scene_cues  \\\n",
       "0             False              False                3               0   \n",
       "1             False              False                1               0   \n",
       "2             False               True              125               0   \n",
       "3             False              False              123              58   \n",
       "4             False              False                0               0   \n",
       "\n",
       "   speaker_lines    chars  \n",
       "0              0   419006  \n",
       "1              2  1218030  \n",
       "2             58   721387  \n",
       "3            840   142317  \n",
       "4              1   561782  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re, pandas as pd\n",
    "\n",
    "CLEAN_DIR = Path(\"../books_clean\")\n",
    "\n",
    "def audit(text: str):\n",
    "    head = text[:5000]\n",
    "    return {\n",
    "        \"has_pg_marker\": bool(re.search(r\"project gutenberg\", text, re.I)),\n",
    "        \"has_license\": bool(re.search(r\"project gutenberg.*license\", text, re.I)),\n",
    "        \"contents_in_head\": bool(re.search(r\"(?:^|\\n)\\s*contents\\s*(?:\\n|:)\", head, re.I)),\n",
    "        \"has_illustrations\": bool(re.search(r\"\\[?illustration[:\\]]\", text, re.I)),\n",
    "        \"stage_dir_count\": len(re.findall(r\"\\[[^\\[\\]\\n]{0,200}\\]\", text)),\n",
    "        \"act_scene_cues\": len(re.findall(r\"^\\s*(ACT|SCENE)\\s+[IVXLC]+\", text, re.I|re.M)),\n",
    "        \"speaker_lines\": len(re.findall(r\"^[A-Z][A-Z '\\-]{2,}[:\\.]$\", text, re.M)),\n",
    "        \"chars\": len(text)\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for p in sorted(CLEAN_DIR.glob(\"*.clean.txt\")):\n",
    "    t = p.read_text(encoding=\"utf-8\")\n",
    "    rows.append({\"file\": p.name, **audit(t)})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aab11796-44f9-40d3-b3a6-5ef54aaa8c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your 02_chunk.ipynb, replace split_sentences with this:\n",
    "\n",
    "def split_sentences(text: str):\n",
    "    \"\"\"\n",
    "    Pluggable sentence splitter:\n",
    "    1) PySBD  2) BlingFire  3) NLTK Punkt  4) Regex fallback\n",
    "    \"\"\"\n",
    "    # 1) PySBD\n",
    "    try:\n",
    "        import pysbd\n",
    "        seg = pysbd.Segmenter(language='en', clean=False)\n",
    "        return seg.segment(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) BlingFire\n",
    "    try:\n",
    "        import blingfire\n",
    "        s = blingfire.text_to_sentences(text)\n",
    "        return [t.strip() for t in s.split('\\n') if t.strip()]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) NLTK Punkt\n",
    "    try:\n",
    "        import nltk\n",
    "        try:\n",
    "            _ = nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "        from nltk.tokenize import sent_tokenize\n",
    "        return sent_tokenize(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4) Regex fallback (no look-behinds; protects common traps)\n",
    "    import re\n",
    "    t = text.replace(\"...\", \"<ELLIPSIS>\")\n",
    "    t = re.sub(r\"(?<=\\d)\\.(?=\\d)\", \"<DECIMAL_DOT>\", t)\n",
    "    # insert EOS markers after sentence-final punct followed by space+capital\n",
    "    t = re.sub(r'([.!?][\"\\')\\]]*)(\\s+)(?=[A-Z])', r'\\1<EOS>\\2', t)\n",
    "    parts = [p.strip() for p in t.split(\"<EOS>\") if p.strip()]\n",
    "    return [p.replace(\"<ELLIPSIS>\", \"...\").replace(\"<DECIMAL_DOT>\", \".\") for p in parts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab8c1c5-ec85-4eba-a244-646450999217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Frankenstein.clean: 1559 chunks → Frankenstein.clean.chunks.csv\n",
      "✔ Mobi Dick.clean: 4194 chunks → Mobi Dick.clean.chunks.csv\n",
      "✔ Pride and Prejudice.clean: 2598 chunks → Pride and Prejudice.clean.chunks.csv\n",
      "✔ Romeo and Juliet.clean: 499 chunks → Romeo and Juliet.clean.chunks.csv\n",
      "✔ The Adventures of Sherlock Holmes.clean: 2168 chunks → The Adventures of Sherlock Holmes.clean.chunks.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>num_chunks</th>\n",
       "      <th>words_total</th>\n",
       "      <th>mean_words_per_chunk</th>\n",
       "      <th>min_words_per_chunk</th>\n",
       "      <th>max_words_per_chunk</th>\n",
       "      <th>target</th>\n",
       "      <th>stride</th>\n",
       "      <th>cap</th>\n",
       "      <th>method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frankenstein.clean</td>\n",
       "      <td>1559</td>\n",
       "      <td>75023</td>\n",
       "      <td>108.04</td>\n",
       "      <td>70</td>\n",
       "      <td>220</td>\n",
       "      <td>120</td>\n",
       "      <td>60</td>\n",
       "      <td>220</td>\n",
       "      <td>sentence+word-pack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mobi Dick.clean</td>\n",
       "      <td>4194</td>\n",
       "      <td>212780</td>\n",
       "      <td>110.48</td>\n",
       "      <td>61</td>\n",
       "      <td>220</td>\n",
       "      <td>120</td>\n",
       "      <td>60</td>\n",
       "      <td>220</td>\n",
       "      <td>sentence+word-pack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pride and Prejudice.clean</td>\n",
       "      <td>2598</td>\n",
       "      <td>127359</td>\n",
       "      <td>108.91</td>\n",
       "      <td>65</td>\n",
       "      <td>220</td>\n",
       "      <td>120</td>\n",
       "      <td>60</td>\n",
       "      <td>220</td>\n",
       "      <td>sentence+word-pack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Romeo and Juliet.clean</td>\n",
       "      <td>499</td>\n",
       "      <td>25946</td>\n",
       "      <td>111.88</td>\n",
       "      <td>66</td>\n",
       "      <td>205</td>\n",
       "      <td>120</td>\n",
       "      <td>60</td>\n",
       "      <td>220</td>\n",
       "      <td>sentence+word-pack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Adventures of Sherlock Holmes.clean</td>\n",
       "      <td>2168</td>\n",
       "      <td>104496</td>\n",
       "      <td>108.08</td>\n",
       "      <td>62</td>\n",
       "      <td>220</td>\n",
       "      <td>120</td>\n",
       "      <td>60</td>\n",
       "      <td>220</td>\n",
       "      <td>sentence+word-pack</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      book  num_chunks  words_total  \\\n",
       "0                       Frankenstein.clean        1559        75023   \n",
       "1                          Mobi Dick.clean        4194       212780   \n",
       "2                Pride and Prejudice.clean        2598       127359   \n",
       "3                   Romeo and Juliet.clean         499        25946   \n",
       "4  The Adventures of Sherlock Holmes.clean        2168       104496   \n",
       "\n",
       "   mean_words_per_chunk  min_words_per_chunk  max_words_per_chunk  target  \\\n",
       "0                108.04                   70                  220     120   \n",
       "1                110.48                   61                  220     120   \n",
       "2                108.91                   65                  220     120   \n",
       "3                111.88                   66                  205     120   \n",
       "4                108.08                   62                  220     120   \n",
       "\n",
       "   stride  cap              method  \n",
       "0      60  220  sentence+word-pack  \n",
       "1      60  220  sentence+word-pack  \n",
       "2      60  220  sentence+word-pack  \n",
       "3      60  220  sentence+word-pack  \n",
       "4      60  220  sentence+word-pack  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summary → ../chunks/chunks_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frankenstein.clean</td>\n",
       "      <td>0</td>\n",
       "      <td>Chapter 1 Chapter 2 Chapter 3 Chapter 4 Chapte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Frankenstein.clean</td>\n",
       "      <td>1</td>\n",
       "      <td>Letter 1 _To Mrs. Saville, England._ St. Peter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frankenstein.clean</td>\n",
       "      <td>2</td>\n",
       "      <td>first task is to assure my dear sister of my w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Frankenstein.clean</td>\n",
       "      <td>3</td>\n",
       "      <td>of Petersburgh, I feel a cold northern breeze ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Frankenstein.clean</td>\n",
       "      <td>4</td>\n",
       "      <td>of promise, my daydreams become more fervent a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 book  chunk_id  \\\n",
       "0  Frankenstein.clean         0   \n",
       "1  Frankenstein.clean         1   \n",
       "2  Frankenstein.clean         2   \n",
       "3  Frankenstein.clean         3   \n",
       "4  Frankenstein.clean         4   \n",
       "\n",
       "                                                text  \n",
       "0  Chapter 1 Chapter 2 Chapter 3 Chapter 4 Chapte...  \n",
       "1  Letter 1 _To Mrs. Saville, England._ St. Peter...  \n",
       "2  first task is to assure my dear sister of my w...  \n",
       "3  of Petersburgh, I feel a cold northern breeze ...  \n",
       "4  of promise, my daydreams become more fervent a...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Chunking pipeline (run after defining split_sentences) ==============\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# --- Config ---\n",
    "CLEAN_DIR = Path(\"../books_clean\")   # run notebook from emobook/notebooks/\n",
    "CHUNK_DIR = Path(\"../chunks\")\n",
    "TARGET_WORDS = 120   # avg words per chunk\n",
    "STRIDE_WORDS = 60    # overlap (words) between consecutive chunks (~50%)\n",
    "CAP_WORDS = 220      # hard cap to avoid very long last sentences inflating size\n",
    "\n",
    "CHUNK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Chunk builders ---\n",
    "def chunk_by_words(sentences, target=TARGET_WORDS, stride=STRIDE_WORDS, cap=CAP_WORDS):\n",
    "    \"\"\"Pack sentences until ~target words; slide forward keeping `stride` words of overlap.\"\"\"\n",
    "    chunks, buf, cur_words = [], [], 0\n",
    "    for s in sentences:\n",
    "        w = len(s.split())\n",
    "        if cur_words + w <= target or not buf:\n",
    "            buf.append(s); cur_words += w\n",
    "        else:\n",
    "            text = \" \".join(buf)\n",
    "            words = text.split()\n",
    "            chunks.append(\" \".join(words[:cap]))\n",
    "            if stride > 0:\n",
    "                keep_words = \" \".join(words[-stride:])\n",
    "                buf = [keep_words, s]\n",
    "                cur_words = len(keep_words.split()) + w\n",
    "            else:\n",
    "                buf = [s]\n",
    "                cur_words = w\n",
    "    if buf:\n",
    "        text = \" \".join(buf)\n",
    "        words = text.split()\n",
    "        chunks.append(\" \".join(words[:cap]))\n",
    "    return chunks\n",
    "\n",
    "def chunk_text_by_words_direct(text: str, target=TARGET_WORDS, stride=STRIDE_WORDS, cap=CAP_WORDS):\n",
    "    \"\"\"Optional: chunk directly by words (no sentence split).\"\"\"\n",
    "    tokens = text.split()\n",
    "    chunks, i, step = [], 0, max(1, target - stride)\n",
    "    while i < len(tokens):\n",
    "        window = tokens[i:i+target]\n",
    "        if not window: break\n",
    "        chunks.append(\" \".join(window[:cap]))\n",
    "        i += step\n",
    "    return chunks\n",
    "\n",
    "# --- Driver ---\n",
    "def build_chunks_for_all(use_sentence_split: bool = True,\n",
    "                         target: int = TARGET_WORDS,\n",
    "                         stride: int = STRIDE_WORDS,\n",
    "                         cap: int = CAP_WORDS):\n",
    "    stats = []\n",
    "    for p in sorted(CLEAN_DIR.glob(\"*.clean.txt\")):\n",
    "        txt = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "        if use_sentence_split:\n",
    "            sents = split_sentences(txt)           # uses your pluggable splitter\n",
    "            chunks = chunk_by_words(sents, target=target, stride=stride, cap=cap)\n",
    "            method = \"sentence+word-pack\"\n",
    "        else:\n",
    "            chunks = chunk_text_by_words_direct(txt, target=target, stride=stride, cap=cap)\n",
    "            method = \"direct-word\"\n",
    "\n",
    "        out_csv = CHUNK_DIR / f\"{p.stem}.chunks.csv\"\n",
    "        pd.DataFrame({\n",
    "            \"book\": p.stem,\n",
    "            \"chunk_id\": range(len(chunks)),\n",
    "            \"text\": chunks\n",
    "        }).to_csv(out_csv, index=False)\n",
    "\n",
    "        n_words = [len(c.split()) for c in chunks] if chunks else []\n",
    "        stats.append({\n",
    "            \"book\": p.stem,\n",
    "            \"num_chunks\": len(chunks),\n",
    "            \"words_total\": len(txt.split()),\n",
    "            \"mean_words_per_chunk\": round(sum(n_words)/len(n_words), 2) if n_words else 0,\n",
    "            \"min_words_per_chunk\": min(n_words) if n_words else 0,\n",
    "            \"max_words_per_chunk\": max(n_words) if n_words else 0,\n",
    "            \"target\": target, \"stride\": stride, \"cap\": cap, \"method\": method\n",
    "        })\n",
    "        print(f\"✔ {p.stem}: {len(chunks)} chunks → {out_csv.name}\")\n",
    "\n",
    "    stats_df = pd.DataFrame(stats)\n",
    "    stats_df.to_csv(CHUNK_DIR / \"chunks_summary.csv\", index=False)\n",
    "    display(stats_df)\n",
    "    print(f\"Saved summary → {CHUNK_DIR / 'chunks_summary.csv'}\")\n",
    "\n",
    "# --- Run it ---\n",
    "build_chunks_for_all(use_sentence_split=True)\n",
    "\n",
    "# --- Optional quick peek ---\n",
    "try:\n",
    "    any_file = next(CHUNK_DIR.glob(\"*.chunks.csv\"))\n",
    "    display(pd.read_csv(any_file).head(5))\n",
    "except StopIteration:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb2aa2-6666-4cb3-a028-001f33957c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
